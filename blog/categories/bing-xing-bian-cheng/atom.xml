<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 并行编程 | Yebangyu's Blog]]></title>
  <link href="http://www.yebangyu.org/blog/categories/bing-xing-bian-cheng/atom.xml" rel="self"/>
  <link href="http://www.yebangyu.org/"/>
  <updated>2016-09-10T19:48:54+08:00</updated>
  <id>http://www.yebangyu.org/</id>
  <author>
    <name><![CDATA[Yebangyu]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Lock Free中的Epoch Based Reclamation]]></title>
    <link href="http://www.yebangyu.org/blog/2016/09/09/epochbasedreclamation/"/>
    <updated>2016-09-09T22:24:02+08:00</updated>
    <id>http://www.yebangyu.org/blog/2016/09/09/epochbasedreclamation</id>
    <content type="html"><![CDATA[<h2 id="section">楔子</h2>

<p>一般认为，用C/C++编写Lock Free代码非常困难，主要原因无非是两个：</p>

<blockquote>
  <ul>
    <li>内存模型</li>
  </ul>
</blockquote>

<blockquote>
  <ul>
    <li>内存回收</li>
  </ul>
</blockquote>

<p>C++11引入了标准的内存模型，在此之前，C++程序员依赖于具体的体系结构特点和编译器提供的feature来保证正确的内存访问语义。C++11出来后，程序员编写健壮的、可移植的lock free代码成为可能。</p>

<p>但是内存回收问题依旧存在。我们知道，和Java这种提供自动gc的语言相比，C++程序员刀耕火种，得自己管理内存。当一个线程正在访问某块内存，而另外一个线程将它释放将是一个灾难行为。解决内存回收问题在lock free里显得更加困难。</p>

<p>目前用于lock free代码的内存回收的经典方法有：Lock Free Reference Counting、Hazard Pointer、Epoch Based Reclamation、Quiescent State Based Reclamation等。<a href="http://www.yebangyu.org/blog/2015/12/10/introduction-to-hazard-pointer/">上回</a>我们简单介绍了Hazard Pointer，本文我们介绍Epoch Based Reclamation方法。据我所知，这是第一篇介绍这个方法的中文资料。</p>

<!--more-->

<h2 id="section-1">概念</h2>

<p>1，逻辑删除和物理删除：逻辑删除仅仅是在逻辑上删除该节点，该节点在被逻辑删除之时可能会有其他线程正在访问它，而逻辑删除之后不会再被线程访问到。逻辑删除不回收内存空间。物理删除则是将对应的内存空间回收。一般逻辑删除对应delete，物理删除对应free或者reclaim。</p>

<p>2，Grace Period：记时间段T＝[t1,t2]，如果t1之前逻辑删除的节点，都可以在t2之后安全的回收，那么称T是一个Grace Period。t2之后保证不会有任何线程会访问在t1之前逻辑删除的节点。</p>

<p>3，临界区：在本文，临界区指的是线程访问共享内存资源的代码段。和传统上所说的临界区意义不同。</p>

<h2 id="section-2">实现</h2>

<p>我们依然先给出实现，再讲原理。注意到这里为了能focus我们的主题，我们刻意简化为四个线程，其中三个读线程，它们对数据都是只读的；而只有一个写线程可以对资源进行改写和删除，因此不需要加锁。</p>

<p>也请注意，以下伪代码只起示范作用，离真正生产环境实现还差很远。尽管如此，我们还是提供了一些实现细节和关键点(参考最后的思考一节)。</p>

<p><code>c++
#define N_THREADS 4 //一共4个线程
bool active[N_THREADS] = {false};
int epoches[N_THREADS] = {0};
int globla_epoch = 0;
vector&lt;int&gt; retire_lists[3];
void read(int thread_id)
{
  active[thread_id] = true;
  epoches[thread_id] = globla_epoch;
  //进入临界区了。可以安全的读取
  //...... 
  //读取完毕，离开临界区
  active[thread_id] = false;
}
void logical_deletion(int thread_id)
{
  active[thread_id] = true;
  epoches[thread_id] = globla_epoch;
  //进入临界区了，这里，我们可以安全的读取
  //好了，假如说我们现在要删除它了。先逻辑删除。
  //而被逻辑删除的tmp指向的节点还不能马上被回收，因此把它加入到对应的retire list
  retire_lists[epoches[thread_id]].push_back(tmp);
  //离开临界区
  active[thread_id] = false;
  //看看能不能物理删除
  try_gc();
}
bool try_gc()
{
  e = global_epoch;
  for (int i = 0; i &lt; N_THREADS; i++) {
    if (active[i] &amp;&amp; epochs[i] != e) {
        //还有部分线程没有更新到最新的全局的epoch值
        //这时候可以回收(e + 1) % 3对应的retire list。
        free((e + 1) % 3);//不是free(e)，也不是free(e-1)。参看下面
        return false;
    }
  }
  //更新global epoch
  e = (e + 1) % 3;
  //更新之后，那些active线程中，部分线程的epoch值可能还是e - 1（模3）
  //那些inactive的线程，之后将读到最新的值，也就是e。
  //不管如何，(e + 1) % 3对应的retire list的那些内存，不会有人再访问到了，可以回收它们了
  //因此epoch的取值需要有三种，仅仅两种是不够的。
  free((e + 1) % 3);//不是free(e)，也不是free(e-1)。参看下面
}
bool free(int epoch)
{
  for each pointer in retire_lists[epoch]
    if (pointer is not NULL)
      delete pointer;
}
</code></p>

<h2 id="section-3">原理</h2>

<p>算法维护了一个全局的epoch(取值为0、1、2)和三个全局的retire_list（每个全局的epoch对应一个retire list, retire list 存放逻辑删除后待回收的节点指针）。除此之外我们为每个线程维护一个局部的active flag和epoch。</p>

<h3 id="section-4">读线程</h3>

<p>首先设置自己的active flag 为true，表明自己需要访问共享内存。然后将自己的局部的epoch设置为全局的epoch值。</p>

<p>这之后线程就进入临界区，可以放心的读取资源了，不用担心内存会被回收。</p>

<p>离开临界区时，将自己的active flag设置为false即可。</p>

<h3 id="section-5">写线程</h3>

<p>这里说的写线程是指会对资源进行删除的线程。首先进行逻辑删除，然后尝试对它进行物理删除，也就是回收。</p>

<p>令全局的epoch值为e。首先检查，如果存在活动线程的局部epoch值不等于全局epoch值，那么可以回收retire_lists[(e+1) % 3]。</p>

<p>如果不存在，那么我们把全局epoch的值更新为(e+1)%3，然后再回收对应的retire list。</p>

<p>有点绕？这里关键是理清线程局部epoch和全局epoch的关系：在这个算法里，任何时刻，全局epoch的值如果为e，那么线程的局部epoch值要么也为e，要么为e-1，不可能为e+1。</p>

<p>所以[epoch, epoch + 2]（模3）构成了一个Grace Period。</p>

<p>当全局epoch的值为1的时候，线程的局部epoch要么是1，要么是0，不可能是2。因此[2,1]构成了一个Grace Period，也就是说2对应的retire list这时候可以放心的回收了。</p>

<p>当全局epoch的值为0的时候，线程的局部epoch要么是0，要么是2，不可能是1。因此[1,0]构成了一个Grace Period，也就是说1对应的retire list这时候可以放心的回收了。</p>

<p>当全局epoch的值为2的时候，线程的局部epoch要么是2，要么是1，不可能是0。因此[0,2]构成了一个Grace Period，也就是说0对应的retire list这时候可以放心的回收了。</p>

<p>也因此，当全局epoch值为2时，如果存在部分活跃线程的局部epoch等于1，那么retire_list[1]将不能立马回收。如果这些活跃线程不退出临界区（在里面不断的读、疯狂的读，或者死了），那么retire_list[1]纪录的那些内存将一直无法回收。</p>

<p>这也就是在Epoch Based Reclamation中，回收操作可能被读延迟的原因所在。</p>

<h2 id="section-6">思考</h2>

<p>1，代码中8、9两行是写一个变量(active[i])，读另外一个变量(global_epoch)，在x86下可能会乱序，这里是否需要一个cpu级别的memory barrier？如果被乱序，有没有影响？</p>

<p>2，<code>retire_lists[epoches[thread_id]].push_back(tmp);</code>能否改为<code>retire_lists[global_epoch].push_back(tmp);</code> ？也就是说这里能否改为使用(当前)全局epoch值？</p>

<p>3，和Hazard Pointer相比，Epoch Based Reclamation 有什么优点和缺点？</p>

<p>4，为什么epoch有三个取值0、1、2？能不能仅使用两个？</p>

<p>5，线程局部变量我们使用了<code>active</code>数组和<code>epoches</code>数组，这里会有性能问题，为什么？提示：<a href="http://www.yebangyu.org/blog/2015/12/30/falsesharing/">False Sharing</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MCS Lock]]></title>
    <link href="http://www.yebangyu.org/blog/2016/08/21/mcslock/"/>
    <updated>2016-08-21T20:25:11+08:00</updated>
    <id>http://www.yebangyu.org/blog/2016/08/21/mcslock</id>
    <content type="html"><![CDATA[<h2 id="section">回顾</h2>

<p><a href="http://www.yebangyu.org/blog/2016/05/26/ticketlock/">上回</a> 我们介绍了Ticket Lock算法，和传统的简单的CAS操作来实现spin lock相比，它提供了不少很好的性质：比如FIFO、没有饥饿等等。</p>

<p>但是根据<a href="https://pdos.csail.mit.edu/papers/linux:lock.pdf">论文</a>，ticket lock的scalability很差。我们不妨简单回顾一下（代码片段也摘自该论文）：</p>

<p><code>c++
struct spinlock_t 
{
  int current_ticket ;
  int next_ticket ;
}
void spin_lock (spinlock_t *lock)
{
  int t = atomic_fetch_and_inc(&amp;lock -&gt; next_ticket );
  while (t != lock-&gt;current_ticket )
  ; /* spin */
}
void spin_unlock (spinlock_t *lock)
{
  lock-&gt;current_ticket++;
}
</code></p>

<!--more-->

<p>正如论文中指出的那样，当某个core上持有锁的线程释放锁时，它将把其他core上的<code>current_ticket</code>对应的cacheline都invalid掉，之前所有等锁的线程都会通过bus来读取最新的cacheline，这将造成“拥堵”(也就是所谓的bus traffic)。由于在绝大多数体系结构下，这些读取请求都会被串行化，one by one的处理，因此<code>spin_lock</code>所需时间将正比于等待锁的线程数目。</p>

<p>本质上，这里的问题还是在于资源共享：所有等锁线程都spin在同一个全局变量上。如果每个线程仅仅spin在本地变量(该线程私有的变量)，那么将有效的提高scalability。本文要介绍的MCS Lock就是这样的思路。</p>

<h2 id="section-1">实现</h2>

<p>这次，我们先给出实现（环境为：GCC 4.8 + X86体系结构 + Ubuntu 14.04 32bit系统），然后再讲解算法。</p>

<p>```c++
#include<pthread.h> //just for test demo.
#define COMPILER_BARRIER() __asm__ __volatile__("" : : : "memory")
#define CPU_RELAX() __asm__ __volatile__("pause\n": : :"memory")
#define CAS(address, exp, target) __sync_bool_compare_and_swap(address, exp, target)
#define ATOMIC_EXCHANGE(address, val) __atomic_exchange_n(address, val, __ATOMIC_SEQ_CST)
//=========================================
struct MCSLockNode
{
  volatile MCSLockNode *volatile next;
  volatile int spin;
};</pthread.h></p>

<p>typedef MCSLockNode MCSLock;
static void lock(MCSLock **m, MCSLockNode *me)
{
  me-&gt;next = NULL;
  me-&gt;spin = 0;
  MCSLockNode *tail = ATOMIC_EXCHANGE(m, me);
  if (!tail) {
    return;
  }
  tail-&gt;next = me;
  COMPILER_BARRIER();
  while (!me-&gt;spin) {
    CPU_RELAX();
  }
  return;
}</p>

<p>static void unlock(MCSLock <em>*m, MCSLockNode *me)
{
  if (!me-&gt;next) {
    if (CAS(m, me, NULL)) {
      return;
    }
    while (!me-&gt;next) {
      CPU_RELAX();
    }
  }
  /</em>It holds that me-&gt;next != NULL here */
  me-&gt;next-&gt;spin = 1;
}
```</p>

<h2 id="section-2">算法</h2>

<p>MCS Lock算法维护一个队列(队尾指针tail)。</p>

<p>lock：尝试加锁的线程创建一个节点，并将其中的<code>spin</code>域设置为0。为0表示不是锁的持有者(lock holder)，接着通过原子操作<code>ATOMIC_EXCHANGE</code>，将自己插入队列中，并获得插入前队尾指针tail。如果tail指针为空，说明当前锁是空闲的，没有被任何线程占用，因此获得锁成功；如果tail指针不空，那么设置自己的前驱，然后自旋，等待它的前驱(线程)将自己的<code>spin</code>域置为1。<code>spin</code>域为1，那么该线程就变成锁的持有者了。</p>

<p>unlock：释放锁的线程需要检查是否存在后继，如果存在后继，那么将它的后继的<code>spin</code>域设置为1即可，后继将结束spin，成功获得锁。一切搞定，如41行代码所示。</p>

<p>如果不存在后继，那么这里需要特别特别注意。不存在后继有两种可能：</p>

<p>它是最后一个线程。33-35行处理的就是这种情况。那么这时候只需要把队列置为空即可。</p>

<p>它不是最后一个线程。那么此时存在另外一个线程在进行加锁操作，尝试加锁的线程已经通过18-21行设置了新的队尾指针，但是还没设置它的前驱，也就是还没执行到22行。因此释放锁的线程需要等待，等待22行被其他线程执行，等待它被人设置为别人的前驱。这也就是36-38行所做的事。</p>

<p>简单说来，MCS Lock就是将线程们用队列管理起来；加锁的线程spin在自己的变量上；释放锁的线程只更新它后继的变量。</p>

<h2 id="section-3">练习</h2>

<p>1，MCS Lock的优点是显而易见的，因为每个等待锁的线程都spin在自己的局部的变量上。以至于新版的linux内核已经开始采用它，来代替之前的ticket lock实现。那么，它的缺点呢？</p>

<p>2，列出表格，从公平性、是否出现饥饿、时间空间复杂度等方面，全面对比ticket lock、mcs lock、peterson lock等。</p>

<p>3，编写程序测试这几个算法的性能，考察它们的scalability。</p>

<h2 id="section-4">附录</h2>

<p>MCS Lock测试程序Demo</p>

<p><code>c++
MCSLock *global_lock = NULL;
void* f(void *arg)
{
  MCSLockNode mcs_node;
  for(int i = 0; i &lt; 1000000; i++) {
    lock(&amp;global_lock, &amp;mcs_node);
    unlock(&amp;global_lock, &amp;mcs_node);
  }
  return NULL;
}
int main()
{
  pthread_t pthread1;
  pthread_t pthread2;  
  pthread_create(&amp;pthread1, NULL, f, NULL);
  pthread_create(&amp;pthread2, NULL, f, NULL);
  pthread_join(pthread1, NULL);  
  pthread_join(pthread2, NULL);  
  return 0;
}
</code></p>

<h2 id="section-5">致谢</h2>

<p>本文刚写作时，在23行使用了一个cpu级别的memory barrier，因为这里我们需要写<code>tail-&gt;next</code>，然后读<code>me-&gt;spin</code>，这两个是不同的变量，而在X86下，写读不同变量可能会被CPU乱序。</p>

<p>本文发出后，不少朋友，包括@ForFunny 、 @赵星宇tju 发起了是否编译器级别的memory barrier即可的讨论。</p>

<p>经过认真讨论和思考，23行改为使用编译器级别的memory barrier。感谢@ForFunny @赵星宇tju参与讨论。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[实现可重入锁]]></title>
    <link href="http://www.yebangyu.org/blog/2016/07/12/reentrantlock/"/>
    <updated>2016-07-12T20:27:25+08:00</updated>
    <id>http://www.yebangyu.org/blog/2016/07/12/reentrantlock</id>
    <content type="html"><![CDATA[<h2 id="section">基本概念</h2>

<p>可重入锁(Reentrant Lock)，是指允许同一个线程多次对该锁进行acquire动作。对于不可重入的锁，当一个线程多次调用acquire后将造成死锁。可重入锁具有广泛的应用，例如：</p>

<p><code>c++
struct Routine
{
  void f()
  {
    lock_.lock();
    h();
    g();
    lock_.unlock();
  }
  void g()
  {
    lock_.lock();
    cout&lt;&lt;"abc"&lt;&lt;endl;
    lock_.unlock();
  }
  void h()
  {
    lock_.lock();
    cout&lt;&lt;"def"&lt;&lt;endl;
    lock_.unlock();
  }
  Lock lock_;
};
</code></p>

<!--more-->

<p>在函数f里调用了g和h，而在每个函数里都试图对lock进行acquire操作。可能你会说保证只有一个函数加锁就行了，但是有时候很难做成这个样子，毕竟一般我们都要求任何一个public函数被调用时候都得保证是线程安全因此需要加锁的。</p>

<p>那么，reentrant lock和recursive lock有什么区别呢？根据<a href="https://en.wikipedia.org/wiki/Reentrant_mutex">wikipedia</a>，这两个是一样的。</p>

<h2 id="section-1">基本实现</h2>

<p>自己实现一个可重入锁并不困难：我们增加两个field，一个用来记录锁当前被哪个线程拥有；一个用来记录尝试acquire的次数。如下所示：</p>

<p><code>c++
#include &lt;stdint.h&gt;
#include &lt;pthread.h&gt;
Class MyReentrantLock
{
public:
  MyReentrantLock()
  {
    lock_holder_ = NULL;
    hold_counter_ = 0;
    pthread_mutex_init(&amp;lock_, NULL);
  }
  int lock()
  {
    int ret = 0;
    pthread_t curr_id = pthread_self();
    if (lock_holder_ == curr_id) {
      ++hold_counter_;
    } else {
      pthread_mutex_lock(&amp;lock_);
      lock_holder_ = curr_id;
      hold_counter_ = 1;
    }
    return ret;
  }
  int unlock()
  {
    int ret = 0;
    pthread_t curr_id = pthread_self();
    if (lock_holder_ != curr_id) {
      ret = -1;
    } else {
      if (--hold_counter_ == 0) {
        lock_holder_ = NULL;
        pthread_mutex_unlock(&amp;lock_);
      }
    }
    return ret;
  }
private:
  pthread_mutex_t lock_;
  pthread_t lock_holder_;
  int64_t hold_counter_;
};
</code></p>

<p>顺便说一句，默认pthread_mutex_t是不可重入的。为了让它可重入，可以这样：</p>

<p><code>c++
pthread_mutex_t mutex;
pthread_mutexattr_t attr;
pthread_mutexattr_init(&amp;attr);
pthread_mutexattr_settype(&amp;attr, PTHREAD_MUTEX_RECURSIVE);//设置可递归也就是可重入
pthread_mutex_init(&amp;mutex, &amp;attr);
</code></p>

<h2 id="section-2">参考文献</h2>

<p>1，《The Art Of Multiprocessor Programming》 8.4节，187页</p>

<p>这本书的作者都是并发编程大仙级人物。虽然是用Java语言写的，但是还是很值得一读。</p>

<p>2，<a href="http://preshing.com/20120305/implementing-a-recursive-mutex/">Implementing a Recursive Mutex</a></p>

<p>很好的博客，每篇都非常高水准。我们最近正在翻译其中和并发编程相关的文章。可以点击这里查看：<a href="http://www.chongh.wiki/categories/High-performance/">深入探索并发编程系列</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sequence Lock]]></title>
    <link href="http://www.yebangyu.org/blog/2016/06/26/sequence-lock/"/>
    <updated>2016-06-26T14:24:59+08:00</updated>
    <id>http://www.yebangyu.org/blog/2016/06/26/sequence-lock</id>
    <content type="html"><![CDATA[<p>在读多写少(Read Mostly)的场景里，我们可能会使用的同步设施包括：</p>

<blockquote>
  <ul>
    <li>Mutex</li>
    <li>Reader Writer Lock</li>
    <li>Sequence Lock</li>
    <li>Read Copy Update</li>
  </ul>
</blockquote>

<p>前面两种一般人都很清楚了，如雷贯耳、妇孺皆知。如果您对mutex的实现细节有兴趣，建议您阅读我的两篇博客：<a href="http://www.yebangyu.org/blog/2016/03/04/petersonalgorithm/">这篇</a>和<a href="http://www.yebangyu.org/blog/2016/05/26/ticketlock/">这篇</a></p>

<p>从今天起，我们介绍后面两种同步设施。今天我们先介绍Sequence Lock。</p>

<!--more-->

<h2 id="section">基本原理</h2>

<p>我们知道，传统的Reader Writer Lock是reader preference的，可能会产生writer starvation。和Reader Writer Lock不同，sequence lock是writer preference的，writer随时都可以更新临界资源。</p>

<p>sequence lock的精髓在于一个sequence count。当writer在更新时，count为奇数；不存在writer更新时，该count为偶数。</p>

<p>count初始化为一个偶数，比如说0。当writer操作临界资源前，先将count++，这时候count变成奇数；然后writer操作临界资源，完毕后，再count++，这时候count将又恢复为偶数。</p>

<p>对于reader，每次进入临界区前读取count值，如果为偶数，说明没有writer存在，那么它可以进入临界区；如果为奇数，那么它需要等待，不断重试，读取count直到count为偶数。进入临界区读取临界资源后，你知道，从reader进入临界区到试图离开临界区这段时间里，可能writer进来了，因此reader需要重新读取count，看和它进入临界区时的count是否相等，不等的话说明此次读取失败，需要重试。</p>

<h2 id="section-1">基本实现</h2>

<p>```c++
#include<mutex> //c++11的mutex
using namespace std;</mutex></p>

<p>struct SequenceLock
{
  volatile uint64_t sequence_count;
  mutex lock; //这把锁是用于writer们互斥的。保证只有一个writer能更新。和reader无关。
};</p>

<p>static void seqlock_init(SequenceLock &amp;sl)
{
  sl.sequence_count = 0;//初始化为偶数
}</p>

<p>static uint64_t read_seqbegin(SequenceLock &amp;sl)
{
  uint64_t sc = 0;
  while(true) {
    sc = sl.sequence_count;
    CPU_MEMORY_BARRIER();
    if (!(sc &amp; 1)) {
      break; //sc是偶数，说明没有writer，返回
    } 
  }
  return sc;
}</p>

<p>static bool reader_need_retry(SequenceLock &amp;sl, uint64_t oldseq)
{
  uint64_t s = 0;
  CPU_MEMORY_BARRIER();
  s = sl.sequence_count;
  return s != oldseq;
}</p>

<p>static void write_seqlock(SequenceLock &amp;sl)
{
  sl.lock.lock();
  ++sl.sequence_count;//让count变为奇数，和读者声明自己的存在
  CPU_MEMORY_BARRIER();
}</p>

<p>static void write_sequnlock(SequenceLock &amp;sl)
{
  CPU_MEMORY_BARRIER();
  ++sl.sequence_count;//让count恢复为偶数
  sl.lock.unlock();
}</p>

<p>```</p>

<h2 id="section-2">深度思考</h2>

<p>1，sequence lock和reader writer lock相比，有什么区别？</p>

<p>最主要的区别，如上所述，就是writer随时随地可以进行更新，不会出现writer starvation的情况。正因为如此，如果update heavily，那么可能造成reader starvation。然而，正如我们一早所说的，sequence lock用于read mostly的situation。因此，reader starvation几乎不会发生。</p>

<p>reader端并不需要加锁，只在极少情况下需要重试而已。因此，从某种角度来说，sequence lock是一种乐观锁。</p>

<p>2，sequence_count声明为<code>uint32_t</code>是否可以？</p>

<p>看你的writer更新的频率。假如你的writer每小时才更新一次，那么一天更新24次，一个月更新720次，一年才262800次，一百年才26280000次，是没有溢出危险的。如果每纳秒更新一次呢？算算看。</p>

<p>我知道，C语言uint32_max（奇数）再自增1之后溢出会回滚到0（偶数）不会影响到程序的正确性，但是好的程序个人认为不应该对此有依赖。</p>

<p>3，使用sequence lock可能会有什么坑？</p>

<p>假如我们的临界资源是这样的：</p>

<p><code>c++
something *p;
</code></p>

<p>reader进入临界区后读取p，存放在自己的变量something *q里，然后返回。之后writer对p进行了free操作；如果reader之后使用<code>*q</code>将发生错误。对于这种情况，需要使用值拷贝语义，或者通过引入<a href="http://www.yebangyu.org/blog/2015/12/10/introduction-to-hazard-pointer/">hazard pointer</a>等其他机制来避免内存释放问题。不仅如此，实际上在临界区里，reader都不可以使用<code>*p</code>，这是因为和reader writer lock不同，sequence lock是不保证writer的不存在的，也就是说在临界区里，是可能随时有writer对p进行释放等操作的（这才需要读者后面的重试操作嘛），这也是它和读写锁的最大不同。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ticket Lock]]></title>
    <link href="http://www.yebangyu.org/blog/2016/05/26/ticketlock/"/>
    <updated>2016-05-26T19:25:15+08:00</updated>
    <id>http://www.yebangyu.org/blog/2016/05/26/ticketlock</id>
    <content type="html"><![CDATA[<h2 id="section">简单回顾</h2>

<p><a href="http://www.yebangyu.org/blog/2016/03/04/petersonalgorithm/">上回</a> 我们介绍了peterson算法来实现spin lock，算法简单，实现简单，但是值得注意和留心的点很多。</p>

<p>粗略说来，peterson算法的主要缺点在于：</p>

<p>1，很难推广到n个线程(n&gt;=3)。原始的算法针对两个线程，如果想应用在多个线程的场景里，需要做一定的修改。</p>

<p>2，peterson算法的动机是仅仅使用load和store来实现互斥访问。然而，我们知道，现代体系结构下，CPU和编译器会对读写操作进行乱序，仅仅依靠读写操作而不使用memory barrier就编写正确的程序非常困难。</p>

<h2 id="ticket-lock">Ticket Lock</h2>

<p>在介绍Ticket Lock之前，我们首先分析一个妇孺皆知、地球人都知道的实现spin lock的方法（伪代码）：</p>

<!--more-->

<p><code>c++
int flag = 0;
void lock()
{
  while (!cas(flag, 0, 1));
  //get lock ! now , flag == 1
}
void unlock()
{
  flag = 0;
}
</code>
上面代码的第四行使用一个CAS原子操作：判断如果flag是否为0，如果为0，将它的值设置为1。判断和设置，所谓的test和set是一个原子操作，返回ture；如果不为0，则不设置，返回false。</p>

<p>这就是大名鼎鼎的<code>test-and-set</code>来实现spin lock。想想看，这种实现方式有什么缺点？</p>

<p>缺点一：不保证公平性，不满足FIFO先来先服务。假如有两个线程在第4行上spin，那么当持有锁的线程释放锁之后，这两个线程谁会成功拿到锁和谁先来spin没有任何直接关系。</p>

<p>缺点二：我们知道，不管CAS操作是否成功，都会产生大量的因为需要保证cache coherency而产生的message，降低性能。因此有一种改进方式：在CAS之前，先读取flag的值，当flag的值为0的时候，再尝试CAS。如下图的伪代码所示：</p>

<p><code>c++
int flag = 0;
void lock()
{
  while(true) {
    if (flag == 0) {
      if (cas(flag, 0, 1)) {
        break;
      }
    }
  }
  //get lock ! now , flag == 1
}
void unlock()
{
  flag = 0;
}
</code></p>

<p>改进后的算法叫做<code>test-and-test-and-set</code>，然而即使经过改进，上面的两个问题依旧存在。在很多场景下，我们希望：</p>

<p>1，保证公平性。</p>

<p>2，原子操作少些，少些，再少些。</p>

<p>3，没有饥饿。</p>

<p>这就涉及到我们要谈到的Ticket Lock。</p>

<p>想想我们去银行排队办业务：先拿个号，然后排队，叫到你的号就是服务你的时候了。没叫到你？老老实实等着吧（这里不考虑关系户走后门插队）。</p>

<p><a href="https://github.com/yebangyu/Soupen/blob/master/src/concurrency/soupen_ticket_spin_lock.h">Soupen</a>中实现了ticket spin lock，这里摘抄如下：</p>

<p><code>
class SoupenTicketSpinLock
{
  public:
    SoupenTicketSpinLock()
    {
      next_id_ = 0;
      service_id_ = 0;
    }
    bool lock()
    {
      uint64_t my_id = FETCH_AND_ADD(&amp;next_id_, 1);
      CPU_BARRIER();
      while(my_id != ACCESS_ONCE(service_id_)) {}
      return true;
    }
    void unlock()
    {
      INC_ATOMIC(&amp;service_id_, 1);
    }
  private:
    uint64_t next_id_;
    uint64_t service_id_;
};
</code>
第11行：获得自己的号，同时把号码递增。其中<code>fetch_and_add</code>是一个原子操作:</p>

<p><code>c++
int fech_and_add(int *p, int inc)
{
  int origin = *p;
  *p += inc;
  return origin;
}
</code>
顺便说一下，也有所谓的<code>add_and_fech</code>：
<code>c++
int add_and_fetch(int *p, int inc)
{
  *p += inc;
  return *p;
}
</code>
区别就在于返回原来的值还是返回更新后的值</p>

<p>第13行：判断是否到自己的号了，否则就一直等待。</p>

<p>第18行：叫下一个人，和下一个人说，轮到你了。</p>

<p>关于<code>CPU_BARRIER</code>、<code>FETCH_AND_ADD</code>、<code>ACCESS_ONCE</code>和<code>INC_ATOMIC</code>的实现，请参考<a href="https://github.com/yebangyu/Soupen/blob/master/src/base/soupen_define.h">Soupen</a>中的相关代码。</p>

<p>公平性：不难看出，先来排队的人将优先获得服务。</p>

<p>性能：不难看出，和之前的不断CAS的版本相比，ticket lock算法中，一次lock调用只有一次原子操作开销。</p>

<p>饥饿：不难看出，每个线程都可以按照自己的排队顺序拿到锁，不会发生饥饿现象。</p>

<p>哈哈，如果过号呢？现实生活里，去银行排队可能因为时间太长不爽闪人了，银行叫人如果连叫三遍还没看到你就会叫下一个人了。</p>

<p>在这里，假如持有锁的线程crash了，来不及调用unlock，那么所有等待锁的线程都将一直spin，除非，哦，除非海量的线程来排队不断自增<code>next_id_</code>导致<code>next_id_</code>溢出回滚，然后重新等于<code>service_id_</code>。</p>

<p>如果持有锁的线程没有crash，它正常释放锁，而叫到的下一个线程之前crash了，那么也会导致所有排队的线程拿不到锁。</p>

<p>顺便说一下，(部分)Linux Kernel的spin lock就是用ticket lock实现的。更多细节可以参考<a href="https://www.ibm.com/developerworks/cn/linux/l-cn-spinlock/">这里</a>。</p>

<p>那么ticket lock是否完美呢？有什么惊人的缺点呢？请看下集。</p>

]]></content>
  </entry>
  
</feed>
