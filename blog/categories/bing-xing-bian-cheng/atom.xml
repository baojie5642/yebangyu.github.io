<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 并行编程 | Yebangyu's Blog]]></title>
  <link href="http://www.yebangyu.org/blog/categories/bing-xing-bian-cheng/atom.xml" rel="self"/>
  <link href="http://www.yebangyu.org/"/>
  <updated>2016-01-30T16:10:49+08:00</updated>
  <id>http://www.yebangyu.org/</id>
  <author>
    <name><![CDATA[Yebangyu]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Spinlock and mutex]]></title>
    <link href="http://www.yebangyu.org/blog/2016/01/24/spinlock-and-mutex/"/>
    <updated>2016-01-24T15:50:31+08:00</updated>
    <id>http://www.yebangyu.org/blog/2016/01/24/spinlock-and-mutex</id>
    <content type="html"><![CDATA[<p>Spinlock（自旋锁）和mutex作为两种互斥锁，在并行编程中都得到了广泛应用。那么，这两种锁有什么区别吗？</p>

<p>当一个线程对Spinlock加锁时，如果该锁被其他线程占用，那么该线程会通过一个loop不断地重试（ try again and again）；而使用mutex的线程没有得到锁时，会sleep。</p>

<p>因为，当临界区较短时，Spinlock因为没有上下文切换，可能性能更优；当临界区较长时，不断的spin将浪费大量的cpu资源。</p>

<!--more-->

<p>如何实现一个Spinlock呢？下面简单封装了一下，并在Ubuntu 14.04 32bit系统，X86体系结构，Intel I5双核处理器环境下，测试相应的性能：</p>

<p><code>c++
#include&lt;thread&gt;
using namespace std;
int counter = 0;
int lock = 0;
void spin_lock()
{
  while(__sync_lock_test_and_set(&amp;lock, 1));
}
void spin_unlock()
{
  __sync_lock_release(&amp;lock);
}
void f()
{
  for (int i = 0; i &lt;  10000000; i++) {
    spin_lock();
    counter = counter + 2;//tiny critical section
    spin_unlock();
  }
}
int main() 
{
  thread t1(f);
  thread t2(f);
  t1.join();
  t2.join();
  return 0;
}
</code></p>

<p>2个线程时，这个程序的运行时间为：</p>

<pre><code>real	0m1.082s
user	0m2.060s
sys	0m0.000s
</code></pre>

<p>4个线程时：</p>

<pre><code>real	0m5.701s
user	0m19.400s
sys	0m0.000s
</code></pre>

<p>如果改为std::mutex(lock和unlock成员函数)呢？对比一下：</p>

<p>2个线程：</p>

<pre><code>real	0m3.081s
user	0m2.796s
sys	0m3.344s
</code></pre>

<p>4个线程：</p>

<pre><code>real	0m5.860s
user	0m6.004s
sys	0m14.936s
</code></pre>

<p>不难发现，由于大量的上下文切换，使用mutex时，花在sys上的时间要远比使用Spinlock的要多。</p>

<h2 id="section">小结</h2>

<p>以下两种情况，应该考虑使用spinlock代替mutex：</p>

<p>1，每个processor上（只）运行一个线程。</p>

<p>2，线程平均等待（spin）时间少于两次上下文切换的开销。</p>

<p>当然，一切都离不开实际的测试和分析。</p>

<p>下次，我们将研究更多、更高效的spinlock实现。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Concurrent and Parallel]]></title>
    <link href="http://www.yebangyu.org/blog/2016/01/17/concurrenyandparallism/"/>
    <updated>2016-01-17T09:49:00+08:00</updated>
    <id>http://www.yebangyu.org/blog/2016/01/17/concurrenyandparallism</id>
    <content type="html"><![CDATA[<p>什么是并发(<strong>Concurrency，Concurrent</strong>)，什么是并行(<strong>parallism，Parallel</strong>)？这两者有什么区别？本文收录一下我听过的、我见过的、我看过的一些人的看法。仅供参考：</p>

<h2 id="paul-butcher">Paul Butcher</h2>

<p><strong>Paul Butcher</strong>在他的《<strong>Seven Concurrency Models in Seven Weeks</strong>
》里开篇就谈到：</p>

<!--more-->

<p>An alternative way of thinking about this is that concurrency is an aspect of
the problem domain—your program needs to handle multiple simultaneous
(or near-simultaneous) events. Parallelism, by contrast, is an aspect of the
solution domain—you want to make your program faster by processing different
portions of the problem in parallel</p>

<p>也就是说，并发是<strong>问题域</strong>，并行是<strong>解决域</strong>。问题是并发的，解决方法是并行的。</p>

<h2 id="rob-pike">Rob Pike</h2>

<p><strong>Rob</strong>是<strong>Go</strong>语言之父，《<strong>The Unix Programming Environment</strong>》 和 《<strong>The Practice of Programming</strong>》（最近正在重读这本小册子）的作者。他有一个经典的解释：</p>

<p>Concurrency is about dealing with lots of things at once.</p>

<p>Parallelism is about doing lots of things at once.</p>

<p>嘿嘿，这个有点意思，不过只能意会了。</p>

<h2 id="paul-e-mckenney">Paul E. McKenney</h2>

<p>又是一个<strong>Paul</strong>。不过这个<strong>Paul</strong>大叔是<strong>IBM</strong>的研究人员，写了一本非常幽默并且有深度的书：《<strong>Is Parallel Programming Hard, And, If So, What Can You Do About It?</strong>》（最近一直在细读，并且做读书笔记，感兴趣的朋友可以参考我的相关<a href="http://www.yebangyu.org/blog/categories/bing-xing-bian-cheng/">博客</a>。） 在这本书（<strong>v2015.01.31a</strong>版本）的第<strong>319</strong>页，有一个附录，介绍 <strong>What is the Difference Between</strong>
“<strong>Concurrent</strong>” <strong>and</strong> “<strong>Parallel</strong>”?时说：</p>

<p>From a classic computing perspective, “concurrent” and
“parallel” are clearly synonyms. However, this has not
stopped many people from drawing distinctions between
the two</p>

<p>说明<strong>Paul</strong>大叔认为，其实这两者是一回事，有些人非得区分。好吧，那就区分吧。这些人是如何区分呢？有两个<strong>perspective</strong>：</p>

<p>The first perspective treats “parallel” as an abbreviation
for “data parallel,” and treats “concurrent” as pretty
much everything else</p>

<p>也就是说，<strong>concurrency</strong>有很强的<strong>interdependencies</strong>，它们之间可能要做各种通信，基于比如说<strong>locks</strong>啊，<strong>transactions</strong>啊，等等同步机制。相比，<strong>parallel</strong>中组件的相互依赖就很少。新浪微博网友 <a href="http://weibo.com/u/1085583241">@小恶魔提利昂</a>就持这种观点，他说：“并发任务处理的时候，会在并发处理时候可以交换信息，有CSP式的，也可以内存共享式的，但是在外部看到的效果是若干核或若干线程/协程同时对应这些并发任务。并行处理的话，同时处理的任务要做到上下文环境都是隔离的。”</p>

<p>Now, this second perspective can be thought of as making
the workload match the available scheduler, with parallel
workloads able to operate on a simple scheduler
and concurrent workloads requiring more sophisticated
schedulers.</p>

<p>恩，第二个角度就是需不需要复杂的<strong>scheduler</strong>。</p>

<p>但是<strong>Paul</strong>大叔说，这两个视角很可能是不可兼得或者说矛盾滴。此话怎讲？</p>

<p>考虑每个<strong>CPU</strong>一个线程的基于<strong>lock</strong>通信的程序。是<strong>Concurrency</strong>吗？从第一个角度讲，是的，用<strong>lock</strong>啊，各种同步各种通信啊。从第二个角度看，又不是。</p>

<p>以上就是<strong>Paul McKenney</strong>大致的观点。</p>

<h2 id="yebangyu">yebangyu</h2>

<p><strong>yebangyu</strong>是博主，<strong>yebangyu.org</strong>公司CEO兼站长兼董事长兼老板，苦逼屌丝底层搬砖码农。恩，就是我了。</p>

<p>个人观点：</p>

<p>1，首先，持<strong>Paul McKenney</strong>的观点，没必要区分这两个词。</p>

<p>2，<strong>Concurrency</strong>这个单词含有类“occur”的词根，表示发生，<strong>con</strong>代表共同、一起，指共同发生的意思。而<strong>parallel</strong>词根是<strong>para</strong>，表示相同的、类似、平行的、差不多的。因为，也可以认为问题是同时发生的，解决方法是平行处理。</p>

<p>3，写书在取书名的时候需要区分。如果你是讲<strong>MPI、Open MP</strong>这类技术，建议用并行或者说<strong>Parallel Computing</strong>。如果是讲<strong>lock free、multi-thread</strong>这些共享内存编程的，建议用<strong>Concurrency</strong>或者<strong>Concurrency Programming</strong>。</p>

<p>那么，<strong>Go</strong>语言这种<strong>CSP</strong>类型的<strong>Channel</strong>的呢？个人认为，都可以吧。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Memory Consistency和Cache Coherence]]></title>
    <link href="http://www.yebangyu.org/blog/2016/01/09/memoryconsistencyandcachecoherence/"/>
    <updated>2016-01-09T22:52:52+08:00</updated>
    <id>http://www.yebangyu.org/blog/2016/01/09/memoryconsistencyandcachecoherence</id>
    <content type="html"><![CDATA[<h2 id="memory-consistency">Memory Consistency</h2>

<p><strong>Memory Consistency</strong>(<strong>MC</strong>)，有时候又叫做<strong>Memory Consistency Model</strong>或者<strong>Memory Model</strong>。为了理解为什么需要引入这种东西，我们首先看以下程序：</p>

<!--more-->

<pre><code>初始：x=0 y=0

Thread1：

S1：x=1

L1：r1=y

Thread2：

S2：y=2

L2：r2=x
</code></pre>

<p>其中，<strong>S1、S2、L1、L2</strong>是语句代号（<strong>BTW</strong>，<strong>S</strong>表示<strong>Store</strong>，<strong>L</strong>表示<strong>Load</strong>）；<strong>r1</strong>和<strong>r2</strong>是两个寄存器。<strong>x</strong>和<strong>y</strong>是两个不同的内存变量。</p>

<p>两个线程执行完之后，<strong>r1</strong>和<strong>r2</strong>可能是什么值？</p>

<p>注意到线程是并发、交替执行的，下面是可能的执行顺序和相应结果：</p>

<p><strong>S1 L1 S2 L2</strong> 那么<strong>r1=0 r2=2</strong></p>

<p><strong>S1 S2 L1 L2</strong> 那么<strong>r1=2 r2=1</strong></p>

<p><strong>S2 L2 S1 L1</strong> 那么<strong>r1=2 r2=0</strong></p>

<p>这些都是意料之内、情理之中的。但是在<strong>x86</strong>体系结构下，很可能得到<strong>r1=0 r2=0</strong>这样的结果。是不是大吃一惊？</p>

<p>如果没有<strong>Memory Consistency</strong>，那么程序员对于自己编写的多线程程序会输出什么将一无所知：天知道会输出什么。</p>

<p>因此，<strong>Memory Consistency</strong>就是程序员（编程语言）、编译器、CPU间的一种协议。这个协议保证了程序访问内存时可能得到什么值，会得到什么值。</p>

<h2 id="sequential-consistency">Sequential Consistency</h2>

<p>在<strong>Sequential Consistency</strong>这种<strong>Memory Model</strong>下，刚才讨论的那个程序不可能输出<strong>r1=0 r2=0</strong>这种结果。怎么说？这就牵涉到一个问题：什么是<strong>Sequential Consistency</strong>（<strong>SC</strong>）。</p>

<p>根据<strong>Leslie Lamport</strong>在<strong>1979</strong>年<strong>9</strong>月发表的论文《<strong>How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs</strong>》里提出的<strong>SC</strong>的定义：</p>

<p>the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program.</p>

<p>根据这个定义，在<strong>SC</strong>模型下，任何<strong>execution</strong>的执行顺序（我们称为<strong>Memory Order</strong>）必须<strong>respect</strong>每个线程的<strong>Program Order</strong>。什么是<strong>Program Order</strong>？对于以上程序，在<strong>Thread1</strong>中，<strong>S1</strong>先于<strong>L1</strong>（不妨记为<strong>S1&lt;L1</strong>）；在<strong>Thread2</strong>中，<strong>S2</strong>先于<strong>L2</strong>（记为<strong>S2&lt;L2</strong>）。这就是<strong>Program Order</strong>。</p>

<p>请时刻注意，<strong>Program Order</strong>只针对某个线程内的语句而言，不涉及到跨线程。比如<strong>Thread1</strong>中的<strong>S1</strong>和<strong>Thread2</strong>中的<strong>L2</strong>，就无所谓什么<strong>Program Order</strong>了。</p>

<p>好了，现在知道为什么在<strong>SC</strong>下，有些结果可能出现，有些不可能了。</p>

<p><strong>S1 L1 S2 L2 r1=0 r2=2</strong> 没问题，没有违背<strong>S1&lt;L1 S2&lt;L2</strong></p>

<p><strong>S1 S2 L1 L2 r1=1 r2=2</strong> 没问题，没有违背<strong>S1&lt;L1 S2&lt;L2</strong></p>

<p><strong>S2 L2 S1 L1 r1=2 r2=0</strong> 没问题，没有违背<strong>S1&lt;L1 S2&lt;L2</strong></p>

<p>而对于<strong>r1=0 r2=0</strong>，在<strong>SC</strong>下，我们找不到一个能<strong>respect Program Order</strong>的<strong>Memory Order</strong>。因为<strong>r1=0</strong>，说明<strong>L1&lt;S2</strong>，<strong>r2=0</strong>说明<strong>L2&lt;S1</strong>；而<strong>S1&lt;L1，S2&lt;L2</strong>，不难看出这里形成了一个环。</p>

<h2 id="cache-coherence">Cache Coherence</h2>

<p>还是先搬出这张图吧:</p>

<p><img src="http://7xnljs.com1.z0.glb.clouddn.com/cpuand%20cache.png" alt="memorycache" /></p>

<p>(图片来源于<strong>Paul</strong>大叔的《<strong>Is Parallel Programming Hard</strong>》这本书第三章)</p>

<p>多个<strong>CPU cores</strong>，每个<strong>core</strong>上有自己的<strong>Cache</strong>。我们知道，<strong>Cache</strong>是部分内存的映射和缓存，或者说，副本。这就带来一个问题：副本一致性。内存只有一个，每个<strong>cpu cores</strong>却有自己的内存副本，如何保证大家看到的内容是一样的、一致的、正确的呢？这就是<strong>Cache Coherence(CC)</strong>要解决的问题。</p>

<h2 id="cache-coherence--vs-memory-consistency">Cache Coherence  VS Memory Consistency</h2>

<p>从以上分析，我们不难看出，<strong>CC</strong>和<strong>MC</strong>涉及的是两个不同层面的东西，解决的是不同的问题，不可混淆。<strong>CC</strong>解决的是副本一致性问题；<strong>MC</strong>保证的是多线程程序访问内存时可以（可能）读到什么值。</p>

<p>两者有联系吗？有。实现<strong>Memory Consistency</strong>时可能会使用到<strong>Cache Coherence</strong>。细节下次我们接着聊。</p>

<h2 id="section">附录</h2>

<p>1，在并行编程中，我们常常有一个问题，或者需求：比如说上面的那个程序，如何保证线程<strong>2</strong>读到的<strong>x</strong>是线程<strong>1</strong>更新（<strong>x=1</strong>）后的<strong>x</strong>的值呢？</p>

<p>如果不加任何同步设施，仅仅考虑上面的程序，那么答案是：无法保证。因为线程的推进速度不同，哪条指令先被执行也一无所知。这个程序重跑几次，也可能输出不同的结果出来。</p>

<p>也就是说，<strong>Memory Model</strong>保证的是，例如，当线程<strong>2</strong>看到<strong>x</strong>等于<strong>1</strong>的时候，线程<strong>1</strong>是否已经执行了<strong>L1</strong>。也就是说，<strong>Memory Model</strong>确保当一件事情发生时，有其他什么事情<strong>has happened</strong>。在<strong>SC</strong>中，当<strong>L1</strong>发生时，说明或者说暗示着，<strong>S1</strong>已经发生了。</p>

<p>2，为什么<strong>x86</strong>下可能得出<strong>r1=0 r2=0</strong>的结果？</p>

<p>因为<strong>S1</strong>是写一个内存变量而<strong>L1</strong>是读取另一个内存变量（两个变量，或者说两个内存地址），这种情况下的写读可能被<strong>CPU</strong>乱序：先执行<strong>L1</strong>，再执行<strong>S1</strong>。（为了性能，所谓的延后写）</p>

<h2 id="section-1">致谢</h2>

<p>本文发出后，微博网友@linyvxiang 指出了其中的一个笔误，非常感谢。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[诡异的程序性能问题]]></title>
    <link href="http://www.yebangyu.org/blog/2015/12/30/falsesharing/"/>
    <updated>2015-12-30T23:06:12+08:00</updated>
    <id>http://www.yebangyu.org/blog/2015/12/30/falsesharing</id>
    <content type="html"><![CDATA[<p>本文所使用的环境是<strong>Ubuntu 14.04 32bit</strong>系统，<strong>Intel I5</strong>处理器，<strong>X86</strong>体系结构</p>

<h2 id="section">提出问题</h2>

<p>如果我说下面的程序存在性能问题，您信吗？</p>

<!--more-->

<p><code>c++
#include&lt;thread&gt;
int32_t global[2]={0};
void f()
{
  for(int i = 0; i &lt; 100000000; i++) {
    ++global[0];
  }
}
void g()
{
  for(int i = 0; i &lt; 100000000; i++) {
    ++global[1];
  }
}
int main()
{
  std::thread thread1(f);
  std::thread thread2(g);
  thread1.join();
  thread2.join();
  return 0;
}
</code>
这个程序，在我的电脑上，运行时间为：</p>

<pre><code>real	0m0.822s
user	0m1.596s
sys     0m0.000s
</code></pre>

<h2 id="section-1">分析问题</h2>

<p>有人说，两个线程分别操作不同的计数器，这么完美的程序，会有性能问题？</p>

<p>答案是：有。</p>

<p>恩，原因在于大名鼎鼎的<strong>false sharing</strong>。如果您看过我以前写的<a href="http://www.yebangyu.org/blog/2015/10/18/hardwareanditshabit/">这篇</a>博客，应该还记得:现在的计算机一般由一个内存、一个<strong>CPU</strong>组成，而包含多个<strong>CPU Cores</strong>和<strong>Cache</strong>。如这幅图所示：</p>

<p><img src="http://7xnljs.com1.z0.glb.clouddn.com/cpuand%20cache.png" alt="memorycache" /></p>

<p><strong>cacheline</strong>是<strong>cache</strong>块单位，一个<strong>cacheline</strong>大小一般在<strong>32</strong>到<strong>256</strong>字节左右。<strong>cacheline</strong>是这张图中不同模块的数据交互元素。</p>

<p>在上面程序中，<strong>global</strong>是两个<strong>4</strong>字节变量构成的数组，大小为<strong>8</strong>字节，很可能被放到同一个<strong>cacheline</strong>里。当运行在<strong>CPU1 Core</strong>上的线程<strong>thread1</strong>修改了<strong>global[0]</strong>时，会让运行在<strong>CPU2 Core</strong>上对应<strong>global[0]</strong>和<strong>global[1]</strong>的<strong>cacheline</strong>失效，因此运行在<strong>CPU2 Core</strong>上的线程<strong>thread2</strong>修改<strong>global[1]</strong>时会发生<strong>cache miss</strong>，接着它访问内存，修改<strong>global[1]</strong>，这也会让<strong>CPU1 Core</strong>中的<strong>cacheline</strong>失效。很明显，这里面会有大量的<strong>cache miss</strong>和为了缓存一致性而花费的通信开销。</p>

<p>因此这种<strong>false sharing</strong>发生在多核、多线程环境中。单核或者单线程不会有<strong>false sharing</strong>问题。</p>

<p>遗憾的是，程序里存在这样的问题，并不容易通过肉眼发现。</p>

<p>幸运的是，这种问题一旦知道，就比较好解决。</p>

<h2 id="section-2">解决问题</h2>

<p>解决方法一：让这两个计数器间隔足够大，让它们不要在同一个<strong>cacheline</strong>里，不就行了么？</p>

<p>恩，定义一个<strong>global[10000]</strong>，然后线程<strong>1</strong>利用<strong>global[0]</strong>，线程<strong>2</strong>利用<strong>global[9999]</strong>，应该就可以了。</p>

<p>什么？这么愚蠢的方法都想得出来？接着往下看。</p>

<p>解决方法二：假如<strong>global</strong>不是一个数组呢？而是包含多个变量的结构体呢(这种情形也很常见)？上面的方法就不灵了吧？</p>

<p>恩，上面的方法不灵了，而且上面的方法太笨。网上有很多资料告诉您怎么定义变量让其<strong>cacheline aligned</strong>，这也是那些博客千篇一律的做法。还有没有其他方法？有。接着往下看。</p>

<p>解决方法三：重点来了。</p>

<p>我们其实可以在线程里使用局部变量！</p>

<p><code>c++
#include&lt;thread&gt;
int32_t global[2] = {0};
void f()
{
  int counter1 = 0;
  for(int i = 0; i &lt; 100000000; i++) {
    ++counter1;
  }
  global[0] = counter1;
}
void g()
{
  int counter2 =0;
  for(int i = 0; i &lt; 100000000; i++) {
    ++counter2;
  }
  global[1] = counter2;
}
int main()
{
  std::thread thread1(f);
  std::thread thread2(g);
  thread1.join();
  thread2.join();
  return 0;
}
</code>
<strong>counter1</strong>和<strong>counter2</strong>在自己的线程栈上，<strong>cacheline</strong>位于对应的<strong>CPU core</strong>里，大家相安无事。只有执行第<strong>9</strong>行和第<strong>17</strong>行时代价可能高点。</p>

<p>这个程序，在我的电脑上运行时间为：</p>

<pre><code>real	0m0.293s
user	0m0.580s
sys     0m0.000s
</code></pre>

<p>解决方法四：</p>

<p><strong>global</strong>神马变量？全局变量。<strong>counter1/counter2</strong>神马变量？局部变量。</p>

<p>有没有一种东东，既有全局的性质，又有局部的效果（线程私有）呢？</p>

<p>恩，如果您看过我以前写的<a href="http://www.yebangyu.org/blog/2015/10/31/linux-parallen-programmming-infrastructure/">这篇</a>博客，就不会对<strong>__thread</strong>感到陌生。对！提供强大<strong>scalability</strong>的利器，就是它了！</p>

<p><code>c++
#include&lt;thread&gt;
__thread int counter = 0;
void f()
{
  for(int i = 0; i &lt; 100000000; i++) {
    ++counter;
  }
}
void g()
{
  for(int i = 0; i &lt; 100000000; i++) {
    ++counter;
  }
}
int main()
{
  std::thread thread1(f);
  std::thread thread2(g);
  thread1.join();
  thread2.join();
  return 0;
}
</code></p>

<p>这个程序在我的电脑上的运行时间为：</p>

<pre><code>real	0m0.325s
user	0m0.644s
sys     0m0.000s
</code></pre>

<p>不过其他线程如何读取到每个计数线程的<strong>counter</strong>呢？不难，但是也不是那么简单，背后涉及到很多问题（其实本文最大的目的是通过<strong>false sharing</strong>，揭示<strong>partition</strong>这种并发编程里最大的设计原则）。我们下次专门聊。</p>

<h2 id="section-3">附录</h2>

<p>1，编译以上多线程程序的时候，请使用：</p>

<pre><code>g++ -pthread -std=c++11 xxx.cpp
</code></pre>

<p>如果没有指定<code>-pthread</code>，那么程序可以编译链接通过，运行时报错：</p>

<p>terminate called after throwing an instance of ‘std::system_error’</p>

<p>what():  Enable multithreading to use std::thread: Operation not permitted</p>

<p>Aborted (core dumped)</p>

<p>2，程序计时我用的是 <code>time ./a.out</code>的方式。</p>

<h2 id="section-4">致谢</h2>

<p>本文发出后，微博网友@Debin_IIE指出了一个笔误。非常感谢。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Lock Free中的Hazard Pointer(中)]]></title>
    <link href="http://www.yebangyu.org/blog/2015/12/14/introduction-to-hazard-pointer/"/>
    <updated>2015-12-14T22:33:01+08:00</updated>
    <id>http://www.yebangyu.org/blog/2015/12/14/introduction-to-hazard-pointer</id>
    <content type="html"><![CDATA[<p>看过<a href="http://www.yebangyu.org/blog/2015/12/10/introduction-to-hazard-pointer/">上篇</a>的朋友，可能会认为：这不就是<strong>Smart Pointer</strong>么？于是可能写出这样的代码：</p>

<!--more-->

<p><code>c++
#include&lt;iostream&gt;
#include&lt;thread&gt;
using namespace std;
class SmartPointer
{
public:
  SmartPointer(int *p)
  {
    pointee_ = p;
    ref_counts_ = 0;
  }
  int *pointee_;
  int ref_counts_;
};
SmartPointer sp(new int(1));
void Reader()
{
  ++sp.ref_counts_;
  cout&lt;&lt;*(sp.pointee_)&lt;&lt;endl;
  --sp.ref_counts_;
}
void Writer()
{
  if (sp.ref_counts_ == 0)
    delete sp.pointee_;
}
int main()
{
  thread t1(Reader);
  thread t2(Reader);
  thread t3(Reader);
  thread t4(Writer);
  t1.join();
  t2.join();
  t3.join();
  t4.join();
  return 0;
}
</code>
然而事实上，这样做是错的。其中的<strong>race condition</strong>请读者自行分析。</p>

<p>那么，<strong>Hazard Pointer</strong>(<strong>HP</strong>)和<strong>Smart Pointer</strong>(<strong>SP</strong>)有什么区别呢？它们的共同点就是管理对应对象的生命周期，然而这两者有本质的区别，<strong>HP</strong>是线程安全的，而<strong>SP</strong>不是。</p>

<p>在<strong>HP</strong>中，每个读线程维护着自己的<strong>HP</strong> <strong>list</strong>，这个<strong>list</strong>，只由该线程写。因此，它是线程安全的。该<strong>list</strong>会（可以）被其他线程读。</p>

<p>每个写线程维护自己的<strong>retire list</strong>，该<strong>retire list</strong>只由该写线程进行读写。由于写线程可以读其他所有读线程的<strong>HP list</strong>，这样，差集（在自己的<strong>retire list</strong>，但是不在所有读线程的<strong>HP list</strong>里的指针），就是可以安全释放的指针。</p>

<p>而<strong>SP</strong>，则被多个线程读写，<strong>18、19</strong>两行也无法做成原子操作，因此，<strong>SP</strong>和<strong>HP</strong>有本质的区别，使用<strong>SP</strong>的程序往往需要搭配使用锁等设施来保证线程安全。</p>
]]></content>
  </entry>
  
</feed>
